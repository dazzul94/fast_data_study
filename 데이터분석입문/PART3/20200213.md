ㅇ Mini-batch Gradient Descent
- Batch Gradient Descent: 모든 Training 데이터에 대하여 전수 Loss 계산
-> 하나의 데이터에 대해 각각의 Loss를 계산하여 더한다.
- Stochastic: 하나의 데이터에 대해서만 Loss를 계산
- mini-batch: n개의 데이터에 대해 Loss를 계산

ㅇ Back Propagation: Loss로부터 거꾸로 한단계씩 미분하여 결과적으로 특정 가중치의 미분값을 구하는 방법

ㅇ Overfitting
- Deep Learning의 목적은 Deep Neural Network를 이용하여 학습을 잘해서 미지의 데이터(Test set)이 들어왔을 때 잘 예측하는 것인데
-> 학습을 잘하자(Training error)를 줄이자 가 곧
예측을 잘하자(Test error)가 말이 될까?
-> 아니다! 왜냐하면 Test error = Training error + Generalization Gap이기 때문이다

* 그래프를 보면
Model Capacity가 너무 작으면(레이어가 너무 적으면) underfitting이고 (아무리 학습을 해도 예측을 못함)
Model Capacity가 너무 크면(레이어가 너무 많으면)
overfitting이 된다.(generalizaion gap이 커진다)

* Overfitting 예시
열심히 모델한테 고양이 학습을 시켰더니
너무 많은 조건을 갖다대버리니까
+ 뚱뚱해서 고양이가 아니다
+ 색깔이 달라서 고양이가 아니다
등등 예측을 잘 못한다 융통성이 없다!

* Overfitting 극복 방법
1. 데이터 수를 늘린다
2. Regulation 정규화
1) L1/L2 regulation(weight decay)
2) Dropout
3) Batch Nomalization